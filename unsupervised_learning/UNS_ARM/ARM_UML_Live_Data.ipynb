{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c28b2be",
   "metadata": {},
   "source": [
    "#  Load Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87bd58ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:14.705275Z",
     "start_time": "2023-10-02T07:10:14.700268Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# config the path of your project\n",
    "sys.path.append(r\"F:\\Lecture\\Project\\Project for Network IDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b5ff84-2866-4505-8830-26019f3be46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:11:56.931344Z",
     "start_time": "2023-10-02T07:11:56.919542Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing packages that are necessary\n",
    "from config import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3f9410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:11:57.722893Z",
     "start_time": "2023-10-02T07:11:57.717661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'....\\\\dataset\\\\used\\\\live_data.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIVE_DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31455ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:11:58.537805Z",
     "start_time": "2023-10-02T07:11:58.448591Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '....\\\\dataset\\\\used\\\\live_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#reading training data to understant existing features and dimentions of the same. \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLIVE_DATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '....\\\\dataset\\\\used\\\\live_data.csv'"
     ]
    }
   ],
   "source": [
    "#reading training data to understant existing features and dimentions of the same. \n",
    "data = pd.read_csv(LIVE_DATASET_PATH)\n",
    "print(data.shape)\n",
    "data.head(10) #printing the top 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735ba7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.987850Z",
     "start_time": "2023-10-02T07:10:16.987850Z"
    }
   },
   "outputs": [],
   "source": [
    "total = len(data)*1.\n",
    "ax=sns.countplot(x=\"label\", data=data)\n",
    "for p in ax.patches:\n",
    "    print(p)\n",
    "    ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.3, p.get_height()+5))\n",
    "\n",
    "#on y axis  from 0 to the total number of rows in the dataframe\n",
    "ax.yaxis.set_ticks(np.linspace(0, total, 11))\n",
    "print(ax.yaxis.get_majorticklocs())\n",
    "# with out changing the position converting to percentage on y axis without changing the positions. \n",
    "ax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()/total))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439fe4b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134ecca",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ce026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.990188Z",
     "start_time": "2023-10-02T07:10:16.990188Z"
    }
   },
   "outputs": [],
   "source": [
    "# description of all features\n",
    "data_features =pd.read_csv(FEATURE_DATASET_PATH, sep=\",\", encoding='cp1252')\n",
    "print(data_features.shape)\n",
    "data_features.head(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3183c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.991186Z",
     "start_time": "2023-10-02T07:10:16.991186Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "cat_feature = data.select_dtypes(include=['category', object]).columns\n",
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80089d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.992198Z",
     "start_time": "2023-10-02T07:10:16.992198Z"
    }
   },
   "outputs": [],
   "source": [
    "# understaing of numerical features/data/variables\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_features = data.drop(['id','label'], axis=1).select_dtypes(include=numerics).columns\n",
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059dc1bc",
   "metadata": {},
   "source": [
    "## Split Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb55fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.993200Z",
     "start_time": "2023-10-02T07:10:16.993200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = data.iloc[:, :43], data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88534e7f",
   "metadata": {},
   "source": [
    "## Dropping Highly Relevant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2606b3",
   "metadata": {},
   "source": [
    "### Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740713d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.994179Z",
     "start_time": "2023-10-02T07:10:16.994179Z"
    }
   },
   "outputs": [],
   "source": [
    "# from the above graphs while understanding the density of numerical data  we see there is high corelation for certain features. \n",
    "# we shall find the highly corelated data and drop the columns to avoid overhead \n",
    "df_corr = X_train.corr()\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.heatmap(df_corr, annot=True, cmap=plt.cm.viridis)\n",
    "plt.savefig('figures/correlation_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f204430",
   "metadata": {},
   "source": [
    "### Drop Features Correlation Above 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc324bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.995176Z",
     "start_time": "2023-10-02T07:10:16.995176Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.graph_utils import *\n",
    "fig_name = \"figures/cor_matrix\"\n",
    "visualize_cor_matrix(fig_name, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2e3fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.996172Z",
     "start_time": "2023-10-02T07:10:16.996172Z"
    }
   },
   "outputs": [],
   "source": [
    "# we shall find the correlation above 0.95 andn shall drop the columns to avoid overhead  \n",
    "high_corr_var=np.where(df_corr>0.95)\n",
    "high_corr_var\n",
    "# for x,y in zip(*high_corr_var):  解包操作符（unpacking operator）\n",
    "#     print(x, y)\n",
    "#calculate correlation betweena ll columns and remove highly correlated one\n",
    "high_corr_var_pairs=[(df_corr.columns[x],df_corr.columns[y]) \n",
    "               for x,y in zip(*high_corr_var)\n",
    "               if x!=y and x<y]\n",
    "high_corr_var_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef5078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.998299Z",
     "start_time": "2023-10-02T07:10:16.997415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "X_train = X_train.drop(to_drop, axis=1)\n",
    "\n",
    "# Print summary of dropped features\n",
    "print(f\"Dropped {len(to_drop)} highly correlated features:\")\n",
    "for feature in to_drop:\n",
    "    print(f\"- {feature}\")\n",
    "    \n",
    "# Print summary of remaining features\n",
    "print(f\"\\n{len(X_train.columns)} features remaining:\")\n",
    "print(X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e131d",
   "metadata": {},
   "source": [
    "## Feature Mapping （category to numarical）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe5e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:16.999573Z",
     "start_time": "2023-10-02T07:10:16.999573Z"
    }
   },
   "outputs": [],
   "source": [
    "#lets find any catagorial data and lable it usinng lable encoder  with out disturbing the shape/dimentions\n",
    "cat_feature = X_train.select_dtypes(include=['category', object]).columns\n",
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671fc06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.001569Z",
     "start_time": "2023-10-02T07:10:17.001569Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_train[cat_feature] = X_train[cat_feature].apply(LabelEncoder().fit_transform)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db17333",
   "metadata": {},
   "source": [
    "## ARM for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fea877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.002495Z",
     "start_time": "2023-10-02T07:10:17.002495Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we shall proceed with ARM for feature selection \n",
    "# for any feature set to be consider - we are fixing the minimum threshold value with the help of \"support and confidance\"\n",
    "#in this case we  fix the support as 30% and confidance as 70%\n",
    "# any set of features meet the above frequent set we shall consider them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1f605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.003654Z",
     "start_time": "2023-10-02T07:10:17.003654Z"
    }
   },
   "outputs": [],
   "source": [
    "# To reduce the time complexity, the dataset is divided into equal parts\n",
    "shuffled = X_train.sample(frac=1)\n",
    "data_42 = np.array_split(shuffled, 42)\n",
    "len(data_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a11e48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.003654Z",
     "start_time": "2023-10-02T07:10:17.003654Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.arm import *\n",
    "\n",
    "col_ruled_sets = []\n",
    "rules_list = []\n",
    "i=1\n",
    "for part in data_42:\n",
    "    \"\"\"find columns of frequent transaction for all the dataset\"\"\"\n",
    "    print(\"===Started dataset \"+ str(i) +\"====\")\n",
    "    #drop id and label\n",
    "    part = part.drop(['id'], axis=1)\n",
    "    print(part.shape)\n",
    "    #create the binary mode data\n",
    "    part_binary = create_arm_data(part)\n",
    "    #Use apriori algorithm to find the subsets of frequent item\n",
    "    result = apriori(part_binary, min_support=0.3, use_colnames=True, max_len=2)\n",
    "        #Create the rule from subsets\n",
    "    arm_rules = create_arm_rule(result)\n",
    "    rules_list.append(arm_rules)\n",
    "    final_columns = arm_rules['rules_sorted'].unique()\n",
    "    #print(final_columns)\n",
    "    col_final = set()\n",
    "    #add each frequent columns to set\n",
    "    for row in final_columns:\n",
    "        for col in row.split(\",\"):\n",
    "            col_final.add(col)\n",
    "    print(col_final)\n",
    "    col_ruled_sets.append(col_final)\n",
    "    print(\"===Completed dataset \"+ str(i) +\"====\")\n",
    "    i+=1\n",
    "\n",
    " # Concatenate all the rules into a single DataFrame\n",
    "rules_df = pd.concat(rules_list, ignore_index=True)\n",
    "\n",
    "# Truncate decimal values to 8 digits\n",
    "for col in rules_df.columns:\n",
    "    if rules_df[col].dtype == 'float64':\n",
    "        rules_df[col] = rules_df[col].apply(lambda x: round(x, 8))  # Keep 8 decimal places\n",
    "        \n",
    "# Write the rules to a CSV file\n",
    "rules_df.to_csv('csvs/association_rules.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d5ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.003654Z",
     "start_time": "2023-10-02T07:10:17.003654Z"
    }
   },
   "outputs": [],
   "source": [
    "#iterate over all the 42 data set to find all possibel columns \n",
    "# When using associative rule mining, we want to identify interesting relationships between the items (features) in our dataset.\n",
    "#The process involves identifying frequent itemsets, which are sets of items that frequently appear together in the data,\n",
    "# and then using those itemsets to generate association rules, which are statements that describe the relationships between\n",
    "# items.\n",
    "col_set = set()\n",
    "for set_i in col_ruled_sets:\n",
    "    for col in set_i:\n",
    "        col_set.add(col)\n",
    "print(len(col_set))\n",
    "col_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd266f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.011229Z",
     "start_time": "2023-10-02T07:10:17.011229Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"txts/feature_selected.txt\", \"w\") as file:\n",
    "    file.write(str(list(col_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9b70d",
   "metadata": {},
   "source": [
    "Apply Blooms Filter betwen Rules genrated from orginal data and Live data (previously any)\n",
    "\n",
    "For an initial round, we still apply blooms filter, code is designed in such a way that it stores the same rules generated from original data, which will nnot have any effect onn the first round. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df37db",
   "metadata": {},
   "source": [
    "# ML Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a312f98",
   "metadata": {},
   "source": [
    "## Construct Dataset with Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf62028",
   "metadata": {},
   "source": [
    "Prepare Extracted train data  from Original Data D\n",
    "\n",
    "2.1. Using the rules repository R obtained in step 1.3, \n",
    "extract the relevant features from the Original Data D and \n",
    "create a new Train data D' containing only these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b49bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.015799Z",
     "start_time": "2023-10-02T07:10:17.015799Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_test[cat_feature] = X_test[cat_feature].apply(LabelEncoder().fit_transform)\n",
    "df_train = X_train[list(col_set)]\n",
    "df_test = X_test[list(col_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cbe26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.018586Z",
     "start_time": "2023-10-02T07:10:17.018586Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2495f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.021580Z",
     "start_time": "2023-10-02T07:10:17.021580Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dd6c4",
   "metadata": {},
   "source": [
    "Train the Machine and Standardize the data\n",
    "\n",
    "4.1. Train a machine learning model on the preprocessed D' obtained in step 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f9a62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.022576Z",
     "start_time": "2023-10-02T07:10:17.022576Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x = df_train.values\n",
    "x_test = df_test.values\n",
    "std_scaler = preprocessing.MinMaxScaler()\n",
    "std_scaler.fit(x)\n",
    "x_scaled = std_scaler.transform(x)\n",
    "df_train = pd.DataFrame(x_scaled)\n",
    "x_scaled_test = std_scaler.transform(x_test)\n",
    "df_test = pd.DataFrame(x_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b6e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.024570Z",
     "start_time": "2023-10-02T07:10:17.024570Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cecb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.027654Z",
     "start_time": "2023-10-02T07:10:17.027654Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"train data shape\", df_train.shape, y_train.shape)\n",
    "print(\"test data shape\", df_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797c2cd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd69a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.028639Z",
     "start_time": "2023-10-02T07:10:17.028639Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, Normalizer\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f755724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.030633Z",
     "start_time": "2023-10-02T07:10:17.030633Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models import lof, cblof, cof, pca, iforest, knn, mcd, ocsvm, abod, hbos, inne\n",
    "# from pyod.models.feature_bagging import FeatureBagging \n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa2126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.033376Z",
     "start_time": "2023-10-02T07:10:17.033376Z"
    }
   },
   "outputs": [],
   "source": [
    "outliers_fraction = 0.05\n",
    "# random_state = np.random.RandomState(42) \n",
    "\n",
    "classifiers = { \n",
    "#         'Angle-based Outlier Detector (ABOD)': abod.ABOD(contamination=outliers_fraction), \n",
    "        'Cluster-based Local Outlier Factor (CBLOF)':cblof.CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=1), \n",
    "#         'Feature Bagging':FeatureBagging(lof.LOF(n_neighbors=35),contamination=outliers_fraction,check_estimator=False,\n",
    "# random_state=random_state), \n",
    "        'Histogram-base Outlier Detection (HBOS)': hbos.HBOS(contamination=outliers_fraction), \n",
    "        'Isolation Forest': iforest.IForest(contamination=outliers_fraction, random_state=1), \n",
    "        'Isolation-based Anomaly Detection Using Nearest-Neighbor Ensembles': inne.INNE(contamination=outliers_fraction, random_state=1)\n",
    "#         'K Nearest Neighbors (KNN)': knn.KNN(contamination=outliers_fraction), \n",
    "#         'Average KNN': knn.KNN(method='mean',contamination=outliers_fraction) \n",
    "} \n",
    "\n",
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be4ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.035342Z",
     "start_time": "2023-10-02T07:10:17.035342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Acc of train: 0.41369\n",
    "# F1_weighted of train: 0.40623\n",
    "# Acc of test: 0.68055\n",
    "# F1_weighted of train: 0.55322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea22f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.036345Z",
     "start_time": "2023-10-02T07:10:17.036345Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train = df_train.iloc[:1000]\n",
    "# y_test = pd.concat([y_test[y_test == 0].head(100), y_test[y_test == 1].head(100)])\n",
    "# df_test = df_test.loc[y_test.index]\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()): \n",
    "    clf.fit(df_train)\n",
    "    y_predict = clf.predict(df_test)\n",
    "    y_predict_tr = clf.predict(df_train)\n",
    "    # compute metrics\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp/(fp+tn)*100\n",
    "    fnr = fn/(fn+tp)*100\n",
    "    far = (fpr+fnr)/2\n",
    "    fpr_te, tpr_te, t_te = roc_curve(y_test, y_predict)\n",
    "    auc_value = auc(fpr_te, tpr_te)\n",
    "    metrics[clf_name].append(accuracy_score(y_test, y_predict))\n",
    "    metrics[clf_name].append(f1_score(y_test, y_predict))\n",
    "    metrics[clf_name].append(auc_value)\n",
    "    metrics[clf_name].append(fpr)\n",
    "    metrics[clf_name].append(fnr)\n",
    "    metrics[clf_name].append(far)\n",
    "    print(f\"Training of {i} {clf_name} finished\")\n",
    "#     print(f'Acc of test: {accuracy_score(y_test, predictions):.5f}')\n",
    "#     print(f\"F1_weighted of train: {f1_score(y_test, predictions, average='weighted'):.5f}\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac0230",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba0ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.037336Z",
     "start_time": "2023-10-02T07:10:17.037336Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "import prettytable\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"F1 Score\", \"AUC\",\"FPR %\",\"FNR %\",\"FAR %\"]\n",
    "for model_name, model_metrics_list in metrics.items():\n",
    "    acc, f1, auc, fpr, fnr, far = model_metrics_list\n",
    "    x.add_row([model_name, \"{0:.4f}\".format(f1), \"{0:.4f}\".format(auc), \"{:.2f}\".format(fpr), \"{:.2f}\".format(fnr), \"{:.2f}\".format(far)])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3b0cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:10:17.038334Z",
     "start_time": "2023-10-02T07:10:17.038334Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file_path = \"csvs/model_evaluation.csv\"\n",
    "table_data = [\n",
    "    [\"Model\", \"F1 Score\", \"AUC\", \"FPR %\", \"FNR %\", \"FAR %\"]\n",
    "]\n",
    "\n",
    "for model_name, model_metrics_list in metrics.items():\n",
    "    table_data.append([model_name] + [round(value, 4) for value in model_metrics_list[1:]])\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(table_data)\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03beb22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384.262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
